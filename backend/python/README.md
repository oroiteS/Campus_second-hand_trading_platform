# Qwen3 Embedding æ™ºèƒ½æ¨èç³»ç»Ÿ

åŸºäº Qwen3 Embedding æ¨¡å‹çš„æ™ºèƒ½æ¨èç³»ç»Ÿï¼Œæ”¯æŒæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å’Œä¸ªæ€§åŒ–æ¨èåŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½åµŒå…¥æ¨¡å‹**: ä½¿ç”¨ Qwen3-Embedding-0.6B æ¨¡å‹ï¼Œåœ¨ MTEB å¤šè¯­è¨€æ’è¡Œæ¦œè¡¨ç°ä¼˜å¼‚
- ğŸŒ **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰ 100+ ç§è¯­è¨€
- ğŸ¯ **æ™ºèƒ½æ¨è**: åŸºäºç”¨æˆ·åå¥½è¿›è¡Œä¸ªæ€§åŒ–æ¨è
- ğŸ“Š **æ–‡æœ¬ç›¸ä¼¼åº¦**: é«˜ç²¾åº¦çš„æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
- ğŸ”§ **æ˜“äºé›†æˆ**: ç®€æ´çš„ API è®¾è®¡ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰é¡¹ç›®

## å®‰è£…ä¾èµ–

ç¡®ä¿ä½ çš„ Python ç‰ˆæœ¬ >= 3.12ï¼Œç„¶åå®‰è£…ä¾èµ–ï¼š

```bash
# ä½¿ç”¨ uvï¼ˆæ¨èï¼‰
uv sync

# æˆ–ä½¿ç”¨ pip
pip install torch>=2.0.0 transformers>=4.51.0 sentence-transformers>=2.7.0 numpy>=1.24.0 scikit-learn>=1.3.0
```

## å¿«é€Ÿå¼€å§‹

### è¿è¡Œæ¼”ç¤º

```bash
python main.py
```

é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ Qwen3-Embedding-0.6B æ¨¡å‹ï¼ˆçº¦ 1.2GBï¼‰ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ã€‚

### åŸºæœ¬ä½¿ç”¨

```python
from main import Qwen3EmbeddingRecommendation

# åˆå§‹åŒ–æ¨èç³»ç»Ÿ
recommender = Qwen3EmbeddingRecommendation()

# æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
query = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
documents = [
    "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é¢†åŸŸ",
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ"
]

results = recommender.find_similar_items(query, documents, top_k=2)
for doc, score in results:
    print(f"ç›¸ä¼¼åº¦: {score:.4f} - {doc}")

# ä¸ªæ€§åŒ–æ¨è
user_preferences = ["å–œæ¬¢ç§‘å¹»ç”µå½±", "å¯¹AIæ„Ÿå…´è¶£"]
candidate_items = [
    "ã€Šé»‘å®¢å¸å›½ã€‹- å…³äºAIçš„ç§‘å¹»ç”µå½±",
    "ã€Šæ³°å¦å°¼å…‹å·ã€‹- ç»å…¸çˆ±æƒ…ç”µå½±"
]

recommendations = recommender.recommend_items(user_preferences, candidate_items)
for item, score in recommendations:
    print(f"æ¨èåº¦: {score:.4f} - {item}")
```

## API æ–‡æ¡£

### Qwen3EmbeddingRecommendation ç±»

#### åˆå§‹åŒ–

```python
recommender = Qwen3EmbeddingRecommendation(model_name="Qwen/Qwen3-Embedding-0.6B")
```

**å‚æ•°:**
- `model_name` (str): Qwen3 embedding æ¨¡å‹åç§°ï¼Œå¯é€‰ï¼š
  - `Qwen/Qwen3-Embedding-0.6B` (é»˜è®¤ï¼Œè½»é‡çº§)
  - `Qwen/Qwen3-Embedding-4B` (å¹³è¡¡æ€§èƒ½)
  - `Qwen/Qwen3-Embedding-8B` (æœ€é«˜æ€§èƒ½)

#### ä¸»è¦æ–¹æ³•

##### encode_texts(texts, prompt_name=None)

å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡è¡¨ç¤ºã€‚

**å‚æ•°:**
- `texts` (List[str]): å¾…ç¼–ç çš„æ–‡æœ¬åˆ—è¡¨
- `prompt_name` (str, å¯é€‰): æç¤ºè¯åç§°ï¼Œå¦‚ "query" ç”¨äºæŸ¥è¯¢æ–‡æœ¬

**è¿”å›:**
- `np.ndarray`: æ–‡æœ¬å‘é‡æ•°ç»„

##### find_similar_items(query, documents, top_k=5)

æ ¹æ®æŸ¥è¯¢æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚

**å‚æ•°:**
- `query` (str): æŸ¥è¯¢æ–‡æœ¬
- `documents` (List[str]): æ–‡æ¡£åˆ—è¡¨
- `top_k` (int): è¿”å›å‰ k ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ

**è¿”å›:**
- `List[Tuple[str, float]]`: (æ–‡æ¡£, ç›¸ä¼¼åº¦åˆ†æ•°) çš„åˆ—è¡¨

##### recommend_items(user_preferences, candidate_items, top_k=5)

åŸºäºç”¨æˆ·åå¥½æ¨èç‰©å“ã€‚

**å‚æ•°:**
- `user_preferences` (List[str]): ç”¨æˆ·åå¥½æè¿°åˆ—è¡¨
- `candidate_items` (List[str]): å€™é€‰ç‰©å“æè¿°åˆ—è¡¨
- `top_k` (int): æ¨èæ•°é‡

**è¿”å›:**
- `List[Tuple[str, float]]`: æ¨èç»“æœåˆ—è¡¨

## åº”ç”¨åœºæ™¯

### 1. ç”µå•†æ¨è

```python
# ç”¨æˆ·åå¥½
user_preferences = ["å–œæ¬¢æˆ·å¤–è¿åŠ¨", "å…³æ³¨å¥åº·ç”Ÿæ´»"]

# å•†å“åˆ—è¡¨
products = [
    "ä¸“ä¸šè·‘æ­¥é‹ - é€‚åˆé•¿è·ç¦»è·‘æ­¥çš„è½»é‡åŒ–è®¾è®¡",
    "ç‘œä¼½å« - ç¯ä¿æè´¨ï¼Œé€‚åˆå®¶åº­å¥èº«",
    "æ¸¸æˆæ‰‹æŸ„ - é«˜ç²¾åº¦æ§åˆ¶ï¼Œæ¸¸æˆä½“éªŒå‡çº§"
]

recommendations = recommender.recommend_items(user_preferences, products)
```

### 2. å†…å®¹æ¨è

```python
# ç”¨æˆ·é˜…è¯»å†å²
reading_history = ["æœºå™¨å­¦ä¹ ç®—æ³•è¯¦è§£", "æ·±åº¦å­¦ä¹ å®æˆ˜"]

# æ–‡ç« åº“
articles = [
    "ç¥ç»ç½‘ç»œä¼˜åŒ–æŠ€å·§",
    "è®¡ç®—æœºè§†è§‰å…¥é—¨",
    "ç¾é£Ÿåˆ¶ä½œæŒ‡å—"
]

user_profile = " ".join(reading_history)
recommendations = recommender.find_similar_items(user_profile, articles)
```

### 3. é—®ç­”ç³»ç»Ÿ

```python
# çŸ¥è¯†åº“
knowledge_base = [
    "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
    "æœºå™¨å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒ",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†"
]

# ç”¨æˆ·é—®é¢˜
user_question = "å¦‚ä½•å¼€å§‹å­¦ä¹ ç¼–ç¨‹ï¼Ÿ"

answers = recommender.find_similar_items(user_question, knowledge_base, top_k=1)
```

## æ€§èƒ½ä¼˜åŒ–

### æ¨¡å‹é€‰æ‹©

æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š

- **Qwen3-Embedding-0.6B**: è½»é‡çº§ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒ
- **Qwen3-Embedding-4B**: å¹³è¡¡æ€§èƒ½å’Œèµ„æºæ¶ˆè€—
- **Qwen3-Embedding-8B**: æœ€é«˜ç²¾åº¦ï¼Œé€‚åˆå¯¹å‡†ç¡®æ€§è¦æ±‚æé«˜çš„åœºæ™¯

### æ‰¹é‡å¤„ç†

```python
# æ‰¹é‡ç¼–ç ä»¥æé«˜æ•ˆç‡
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
embeddings = recommender.encode_texts(texts)
```

### ç¼“å­˜æœºåˆ¶

å¯¹äºé¢‘ç¹æŸ¥è¯¢çš„æ–‡æœ¬ï¼Œå»ºè®®å®ç°ç¼“å­˜æœºåˆ¶ï¼š

```python
import pickle

# ä¿å­˜ç¼–ç ç»“æœ
embeddings = recommender.encode_texts(documents)
with open('embeddings_cache.pkl', 'wb') as f:
    pickle.dump(embeddings, f)

# åŠ è½½ç¼“å­˜
with open('embeddings_cache.pkl', 'rb') as f:
    cached_embeddings = pickle.load(f)
```

## æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡è¿è¡Œ**: éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š
2. **å†…å­˜éœ€æ±‚**: 0.6B æ¨¡å‹çº¦éœ€ 2GB å†…å­˜ï¼Œ4B æ¨¡å‹çº¦éœ€ 8GB å†…å­˜
3. **GPU åŠ é€Ÿ**: å¦‚æœ‰ GPUï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä½¿ç”¨ GPU åŠ é€Ÿ
4. **æ–‡æœ¬é•¿åº¦**: æ”¯æŒæœ€å¤§ 32K tokens çš„æ–‡æœ¬é•¿åº¦

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: æ¨¡å‹ä¸‹è½½å¤±è´¥**
A: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–å°è¯•ä½¿ç”¨ä»£ç†ã€‚ä¹Ÿå¯ä»¥æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ã€‚

**Q: å†…å­˜ä¸è¶³**
A: å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆ0.6Bï¼‰ï¼Œæˆ–å¢åŠ ç³»ç»Ÿå†…å­˜ã€‚

**Q: è¿è¡Œé€Ÿåº¦æ…¢**
A: ç¡®ä¿å®‰è£…äº† GPU ç‰ˆæœ¬çš„ PyTorchï¼Œæˆ–ä½¿ç”¨æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡ã€‚

**Q: transformers ç‰ˆæœ¬é”™è¯¯**
A: ç¡®ä¿ transformers >= 4.51.0ï¼Œè¿è¡Œ `pip install transformers>=4.51.0` æ›´æ–°ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº Apache 2.0 è®¸å¯è¯å¼€æºã€‚Qwen3 æ¨¡å‹éµå¾ªå…¶å®˜æ–¹è®¸å¯è¯æ¡æ¬¾ã€‚

## å‚è€ƒèµ„æº

- [Qwen3 å®˜æ–¹æ–‡æ¡£](https://qwenlm.github.io/blog/qwen3/) <mcreference link="https://qwenlm.github.io/blog/qwen3/" index="4">4</mcreference>
- [Qwen3-Embedding GitHub](https://github.com/QwenLM/Qwen3-Embedding) <mcreference link="https://github.com/QwenLM/Qwen3-Embedding" index="1">1</mcreference>
- [Hugging Face æ¨¡å‹é¡µé¢](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) <mcreference link="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B" index="3">3</mcreference>