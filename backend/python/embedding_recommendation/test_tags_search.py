#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•çš„æ ‡ç­¾æœç´¢æµ‹è¯•ç¨‹åº
ç”¨äºæµ‹è¯•ChromaDBå‘é‡å­˜å‚¨å’Œæœç´¢åŠŸèƒ½
"""

import os
import chromadb
from dotenv import load_dotenv
import requests
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import jieba
from collections import Counter

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RerankerModel:
    """é‡æ’æ¨¡å‹ç±»ï¼Œç”¨äºå¯¹æœç´¢ç»“æœè¿›è¡ŒäºŒæ¬¡æ’åº"""
    
    def __init__(self):
        # åˆå§‹åŒ–ä¸­æ–‡åˆ†è¯å™¨
        jieba.initialize()
    
    def calculate_text_similarity(self, query, document):
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆåŸºäºè¯æ±‡é‡å å’Œè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰"""
        # åˆ†è¯
        query_words = set(jieba.cut(query.lower()))
        doc_words = set(jieba.cut(document.lower()))
        
        # è®¡ç®—è¯æ±‡é‡å åº¦
        intersection = query_words.intersection(doc_words)
        union = query_words.union(doc_words)
        
        if len(union) == 0:
            jaccard_similarity = 0
        else:
            jaccard_similarity = len(intersection) / len(union)
        
        # è®¡ç®—å­—ç¬¦çº§åˆ«ç›¸ä¼¼åº¦
        query_chars = set(query.lower())
        doc_chars = set(document.lower())
        char_intersection = query_chars.intersection(doc_chars)
        char_union = query_chars.union(doc_chars)
        
        if len(char_union) == 0:
            char_similarity = 0
        else:
            char_similarity = len(char_intersection) / len(char_union)
        
        # è®¡ç®—é•¿åº¦ç›¸ä¼¼åº¦
        len_similarity = 1 - abs(len(query) - len(document)) / max(len(query), len(document), 1)
        
        # ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•°
        combined_score = 0.4 * jaccard_similarity + 0.3 * char_similarity + 0.3 * len_similarity
        
        return combined_score
    
    def rerank_results(self, query, results, embedding_similarities):
        """é‡æ’æœç´¢ç»“æœ"""
        if not results['documents'] or not results['documents'][0]:
            return results
        
        documents = results['documents'][0]
        distances = results['distances'][0]
        
        # è®¡ç®—é‡æ’åˆ†æ•°
        reranked_items = []
        for i, (doc, distance) in enumerate(zip(documents, distances)):
            # åŸå§‹åµŒå…¥ç›¸ä¼¼åº¦ï¼ˆè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
            embedding_sim = 1 - distance
            
            # æ–‡æœ¬ç›¸ä¼¼åº¦
            text_sim = self.calculate_text_similarity(query, doc)
            
            # ç»¼åˆåˆ†æ•°ï¼ˆå¯ä»¥è°ƒæ•´æƒé‡ï¼‰
            final_score = 0.65 * embedding_sim + 0.35 * text_sim
            
            reranked_items.append({
                'document': doc,
                'distance': distance,
                'embedding_similarity': embedding_sim,
                'text_similarity': text_sim,
                'final_score': final_score,
                'original_index': i
            })
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        reranked_items.sort(key=lambda x: x['final_score'], reverse=True)
        
        # é‡æ–°æ„å»ºç»“æœ
        reranked_results = {
            'documents': [[item['document'] for item in reranked_items]],
            'distances': [[1 - item['final_score'] for item in reranked_items]],
            'metadatas': results.get('metadatas', [[]]),
            'ids': results.get('ids', [[]])
        }
        
        return reranked_results, reranked_items

class OllamaEmbeddingFunction:
    def __init__(self, api_base, model_name):
        self.api_base = api_base
        self.model_name = model_name

    def __call__(self, input):
        if not input:
            return []
        
        # ç¡®ä¿inputæ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(input, str):
            input = [input]
        
        texts = input
        
        embeddings = []
        
        # é€ä¸ªå¤„ç†æ¯ä¸ªæ–‡æœ¬
        for text in texts:
            headers = {"Content-Type": "application/json"}
            payload = {"prompt": text, "model": self.model_name}
            
            try:
                response = requests.post(f"{self.api_base}/embeddings", headers=headers, json=payload)
                response.raise_for_status()
                
                json_response = response.json()
                if 'embedding' in json_response:
                    embeddings.append(json_response['embedding'])
                else:
                    print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°åµŒå…¥æ•°æ®ï¼Œå“åº”: {json_response}")
                    # è¿”å›ä¸€ä¸ªé»˜è®¤çš„åµŒå…¥å‘é‡ï¼ˆå…¨é›¶å‘é‡ï¼‰
                    embeddings.append([0.0] * 384)  # å‡è®¾384ç»´
            except Exception as e:
                print(f"è°ƒç”¨Ollama APIå¤±è´¥: {e}")
                # è¿”å›ä¸€ä¸ªé»˜è®¤çš„åµŒå…¥å‘é‡
                embeddings.append([0.0] * 384)
        
        return embeddings

def test_search():
    """æµ‹è¯•æœç´¢åŠŸèƒ½ï¼ˆå¸¦é‡æ’æ¨¡å‹ï¼‰"""
    print("=== å¼€å§‹æµ‹è¯•æ ‡ç­¾æœç´¢åŠŸèƒ½ï¼ˆä½¿ç”¨é‡æ’æ¨¡å‹ï¼‰===")
    
    # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
    client = chromadb.PersistentClient(path="tags_vector_db/")
    
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    api_base = os.getenv('OPENAI_API_BASE', 'http://localhost:11434/api')
    model_name = os.getenv('OPENAI_EMBEDDING_MODEL_NAME', 'qwen3-embedding-0.6b:latest')
    
    # åˆ›å»ºåµŒå…¥å‡½æ•°å’Œé‡æ’æ¨¡å‹
    embedding_func = OllamaEmbeddingFunction(api_base, model_name)
    reranker = RerankerModel()
    
    try:
        # è·å–å·²å­˜åœ¨çš„é›†åˆ
        collection = client.get_collection(name="tags_collection", embedding_function=embedding_func)
        print(f"æˆåŠŸè¿æ¥åˆ°é›†åˆï¼ŒåŒ…å« {collection.count()} ä¸ªæ ‡ç­¾")
        
        # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
        test_queries = [
            "ç”µè„‘",
            "æ‰‹æœº",
            "æ•°å­¦",
            "ä¹¦ç±",
            "è¿åŠ¨",
            "éŸ³ä¹",
            "æ¸¸æˆ",
            "å­¦ä¹ "
        ]
        
        print("\n=== å¼€å§‹æœç´¢æµ‹è¯• ===")
        for query in test_queries:
            print(f"\næœç´¢æŸ¥è¯¢: '{query}'")
            try:
                # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢ï¼ˆè·å–æ›´å¤šç»“æœç”¨äºé‡æ’ï¼‰
                results = collection.query(
                    query_texts=[query],
                    n_results=10  # è·å–æ›´å¤šç»“æœç”¨äºé‡æ’
                )
                
                if results['documents'] and results['documents'][0]:
                    print("\nåŸå§‹æœç´¢ç»“æœ:")
                    for i, (doc, distance) in enumerate(zip(results['documents'][0][:3], results['distances'][0][:3])):
                        print(f"  {i+1}. {doc} (åµŒå…¥ç›¸ä¼¼åº¦: {1-distance:.3f})")
                    
                    # ä½¿ç”¨é‡æ’æ¨¡å‹é‡æ–°æ’åº
                    reranked_results, reranked_items = reranker.rerank_results(query, results, results['distances'][0])
                    
                    print("\né‡æ’åç»“æœ:")
                    for i, item in enumerate(reranked_items[:5]):
                        print(f"  {i+1}. {item['document']}")
                        print(f"     åµŒå…¥ç›¸ä¼¼åº¦: {item['embedding_similarity']:.3f}")
                        print(f"     æ–‡æœ¬ç›¸ä¼¼åº¦: {item['text_similarity']:.3f}")
                        print(f"     ç»¼åˆåˆ†æ•°: {item['final_score']:.3f}")
                        print()
                else:
                    print("  æœªæ‰¾åˆ°ç›¸ä¼¼æ ‡ç­¾")
                    
            except Exception as e:
                print(f"  æœç´¢å¤±è´¥: {e}")
        
        print("\n=== æµ‹è¯•å®Œæˆ ===")
        
    except Exception as e:
        print(f"æ— æ³•è¿æ¥åˆ°é›†åˆ: {e}")
        print("è¯·å…ˆè¿è¡Œ simple_tags_vector.py æ¥åˆ›å»ºå‘é‡æ•°æ®åº“")

def show_all_tags():
    """æ˜¾ç¤ºæ‰€æœ‰å­˜å‚¨çš„æ ‡ç­¾"""
    print("\n=== æ˜¾ç¤ºæ‰€æœ‰å­˜å‚¨çš„æ ‡ç­¾ ===")
    
    client = chromadb.PersistentClient(path="tags_vector_db/")
    
    try:
        collection = client.get_collection(name="tags_collection")
        
        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = collection.get()
        
        if all_docs['documents']:
            print(f"æ€»å…±å­˜å‚¨äº† {len(all_docs['documents'])} ä¸ªæ ‡ç­¾:")
            for i, doc in enumerate(all_docs['documents'][:20]):  # åªæ˜¾ç¤ºå‰20ä¸ª
                print(f"  {i+1}. {doc}")
            
            if len(all_docs['documents']) > 20:
                print(f"  ... è¿˜æœ‰ {len(all_docs['documents']) - 20} ä¸ªæ ‡ç­¾")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡ç­¾")
            
    except Exception as e:
        print(f"æ— æ³•è·å–æ ‡ç­¾åˆ—è¡¨: {e}")

def compare_search_methods(query):
    """æ¯”è¾ƒåŸå§‹æœç´¢å’Œé‡æ’æœç´¢çš„æ•ˆæœ"""
    print(f"\n=== æœç´¢æ–¹æ³•å¯¹æ¯”: '{query}' ===")
    
    client = chromadb.PersistentClient(path="tags_vector_db/")
    
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    api_base = os.getenv('OPENAI_API_BASE', 'http://localhost:11434/api')
    model_name = os.getenv('OPENAI_EMBEDDING_MODEL_NAME', 'qwen3-embedding-0.6b:latest')
    
    # åˆ›å»ºåµŒå…¥å‡½æ•°å’Œé‡æ’æ¨¡å‹
    embedding_func = OllamaEmbeddingFunction(api_base, model_name)
    reranker = RerankerModel()
    
    try:
        collection = client.get_collection(name="tags_collection", embedding_function=embedding_func)
        results = collection.query(
            query_texts=[query],
            n_results=10
        )
        
        if results['documents'] and results['documents'][0]:
            print("\nğŸ“Š åŸå§‹åµŒå…¥æœç´¢ç»“æœ:")
            for i, (doc, distance) in enumerate(zip(results['documents'][0][:5], results['distances'][0][:5])):
                print(f"  {i+1}. {doc} (ç›¸ä¼¼åº¦: {1-distance:.3f})")
            
            # é‡æ’ç»“æœ
            reranked_results, reranked_items = reranker.rerank_results(query, results, results['distances'][0])
            
            print("\nğŸ¯ é‡æ’æ¨¡å‹ä¼˜åŒ–ç»“æœ:")
            for i, item in enumerate(reranked_items[:5]):
                improvement = "ğŸ“ˆ" if item['final_score'] > item['embedding_similarity'] else "ğŸ“‰" if item['final_score'] < item['embedding_similarity'] else "â¡ï¸"
                print(f"  {i+1}. {item['document']} {improvement}")
                print(f"     ç»¼åˆåˆ†æ•°: {item['final_score']:.3f} (åµŒå…¥: {item['embedding_similarity']:.3f}, æ–‡æœ¬: {item['text_similarity']:.3f})")
        else:
            print("  æœªæ‰¾åˆ°ç›¸ä¼¼æ ‡ç­¾")
            
    except Exception as e:
        print(f"æœç´¢å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ChromaDB æ ‡ç­¾æœç´¢æµ‹è¯•ç¨‹åº (å¸¦é‡æ’æ¨¡å‹)")
    print("=" * 50)
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. æµ‹è¯•æœç´¢åŠŸèƒ½ (ä½¿ç”¨é‡æ’æ¨¡å‹)")
        print("2. æ˜¾ç¤ºæ‰€æœ‰æ ‡ç­¾")
        print("3. è‡ªå®šä¹‰æœç´¢ (å¯¹æ¯”é‡æ’æ•ˆæœ)")
        print("4. æœç´¢æ–¹æ³•æ•ˆæœå¯¹æ¯”")
        print("5. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            test_search()
        elif choice == '2':
            show_all_tags()
        elif choice == '3':
            query = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯: ").strip()
            if query:
                print(f"\næœç´¢æŸ¥è¯¢: '{query}'")
                client = chromadb.PersistentClient(path="tags_vector_db/")
                
                # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
                api_base = os.getenv('OPENAI_API_BASE', 'http://localhost:11434/api')
                model_name = os.getenv('OPENAI_EMBEDDING_MODEL_NAME', 'qwen3-embedding-0.6b:latest')
                
                # åˆ›å»ºåµŒå…¥å‡½æ•°å’Œé‡æ’æ¨¡å‹
                embedding_func = OllamaEmbeddingFunction(api_base, model_name)
                reranker = RerankerModel()
                
                try:
                    collection = client.get_collection(name="tags_collection", embedding_function=embedding_func)
                    results = collection.query(
                        query_texts=[query],
                        n_results=15
                    )
                    
                    if results['documents'] and results['documents'][0]:
                        print("\n=== åŸå§‹æœç´¢ç»“æœ ===")
                        for i, (doc, distance) in enumerate(zip(results['documents'][0][:8], results['distances'][0][:8])):
                            print(f"  {i+1}. {doc} (åµŒå…¥ç›¸ä¼¼åº¦: {1-distance:.3f})")
                        
                        # ä½¿ç”¨é‡æ’æ¨¡å‹
                        reranked_results, reranked_items = reranker.rerank_results(query, results, results['distances'][0])
                        
                        print("\n=== é‡æ’åç»“æœ ===")
                        for i, item in enumerate(reranked_items[:8]):
                            print(f"  {i+1}. {item['document']}")
                            print(f"     åµŒå…¥ç›¸ä¼¼åº¦: {item['embedding_similarity']:.3f}")
                            print(f"     æ–‡æœ¬ç›¸ä¼¼åº¦: {item['text_similarity']:.3f}")
                            print(f"     ç»¼åˆåˆ†æ•°: {item['final_score']:.3f}")
                            if i < 7:  # ä¸åœ¨æœ€åä¸€ä¸ªé¡¹ç›®åæ·»åŠ ç©ºè¡Œ
                                print()
                    else:
                        print("  æœªæ‰¾åˆ°ç›¸ä¼¼æ ‡ç­¾")
                except Exception as e:
                    print(f"æœç´¢å¤±è´¥: {e}")
        elif choice == '4':
            query = input("è¯·è¾“å…¥è¦å¯¹æ¯”çš„æœç´¢å…³é”®è¯: ").strip()
            if query:
                compare_search_methods(query)
        elif choice == '5':
            print("é€€å‡ºç¨‹åº")
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main()